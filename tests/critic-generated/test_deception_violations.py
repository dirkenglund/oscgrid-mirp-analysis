"""
DECEPTION CRITIC TESTS
Generated by: Deception Detection Protocol
Target: 20260204_OscGrid_Photonic_Quantum_Analysis.py
Purpose: Detect mocks, placeholders, unverified implementations, and fake computation

CLAUDE.md Rules Enforced:
- No mockups or placeholders
- All implementations must be real and verifiable
- External dependencies must be validated
- Claims must match actual functionality
- No TODO/FIXME without implementation
"""

import pytest
import numpy as np
from pathlib import Path
import re
import ast


class TestDeceptionViolations:
    """Test suite to catch deception patterns in quantum analysis notebook"""

    @pytest.fixture
    def notebook_path(self):
        """Path to the notebook under test"""
        return Path(__file__).parent.parent.parent / "20260204_OscGrid_Photonic_Quantum_Analysis.py"

    @pytest.fixture
    def notebook_source(self, notebook_path):
        """Load notebook source code"""
        with open(notebook_path, 'r') as f:
            return f.read()

    def test_no_simulation_placeholders(self, notebook_source):
        """
        CLAUDE.md Rule: Never accept mockups or simulations as real implementation
        Found: Line 284 - "For development without optical hardware, simulate MiRP dynamics"
        
        VIOLATION: Documentation admits to simulation instead of real optical hardware
        """
        # Check for simulation-related deception
        simulation_pattern = re.compile(
            r'(simulate|mock|fake|placeholder|stub).*dynamics',
            re.IGNORECASE
        )
        matches = simulation_pattern.findall(notebook_source)
        
        assert len(matches) == 0, (
            f"DECEPTION DETECTED: Found {len(matches)} simulation placeholders. "
            f"Must use real optical hardware validation or explicit disclaimer. "
            f"Matches: {matches}"
        )

    def test_mirp_dynamics_function_exists_and_computes(self, notebook_source):
        """
        CLAUDE.md Rule: Verify actual vs claimed functionality
        Found: Lines 290-314 - mirp_dynamics function definition in markdown
        
        VIOLATION: Function may be defined but never executed/validated
        """
        # Check if function definition exists in markdown code blocks
        has_mirp_dynamics_definition = 'def mirp_dynamics' in notebook_source
        has_compute_output_definition = 'def compute_output' in notebook_source
        
        # These are in markdown code blocks, not actual executable code
        # This is DECEPTION if claimed as implementation
        if has_mirp_dynamics_definition or has_compute_output_definition:
            # Parse the actual Python AST (not markdown)
            try:
                tree = ast.parse(notebook_source)
                function_names = [node.name for node in ast.walk(tree) if isinstance(node, ast.FunctionDef)]
                
                # These functions should NOT be in executable code if they're just examples
                is_in_markdown_only = 'mirp_dynamics' not in function_names and 'compute_output' not in function_names
                
                assert not is_in_markdown_only, (
                    "DECEPTION: mirp_dynamics and compute_output shown as code examples in markdown "
                    "but never actually implemented or executed. This is a mockup disguised as implementation."
                )
            except SyntaxError:
                # Can't parse because it's mixed markdown/code - that's actually OK for a marimo notebook
                pass

    def test_no_unverified_external_calls(self, notebook_source):
        """
        CLAUDE.md Rule: Verify external dependencies exist and are callable
        Found: Lines 288, 309 - scipy.integrate.odeint, np.interp referenced in markdown
        
        VIOLATION: External library calls must be validated to work with actual data
        """
        # Check that imports exist
        assert 'import numpy as np' in notebook_source, (
            "DECEPTION: numpy claimed but not imported"
        )
        
        # Check if scipy is mentioned but not imported
        mentions_scipy = 'scipy.integrate' in notebook_source or 'odeint' in notebook_source
        imports_scipy = 'from scipy.integrate import odeint' in notebook_source or 'import scipy' in notebook_source
        
        if mentions_scipy and not imports_scipy:
            pytest.fail(
                "DECEPTION: scipy.integrate.odeint referenced but never imported. "
                "This is a mockup example, not executable code."
            )

    def test_physical_constants_are_real_not_mocked(self, notebook_source):
        """
        CLAUDE.md Rule: No hardcoded mock values masquerading as real physics
        Found: Lines 481-497 - Physical constants definition
        
        VIOLATION: Constants must be scientifically accurate, not approximations
        """
        # Extract actual constant values from executable code
        # Look for h = 6.626e-34 pattern
        h_pattern = re.search(r'h\s*=\s*([\d.e-]+)', notebook_source)
        c_pattern = re.search(r'\bc\s*=\s*([\d.e+]+)', notebook_source)
        
        if h_pattern:
            h_value = float(h_pattern.group(1))
            # Allow some tolerance for floating point, but should be close to CODATA value
            assert abs(h_value - 6.626e-34) / 6.626e-34 < 0.01, (
                f"DECEPTION: Planck's constant h = {h_value} is not accurate. "
                f"Should be ~6.626e-34 J·s (CODATA 2018: 6.62607015e-34)"
            )
        
        if c_pattern:
            c_value = float(c_pattern.group(1))
            # Speed of light should be exact
            assert abs(c_value - 3e8) / 3e8 < 0.01, (
                f"DECEPTION: Speed of light c = {c_value} is not accurate. "
                f"Should be 3e8 m/s (exact: 299792458 m/s)"
            )

    def test_no_todo_fixme_without_implementation(self, notebook_source):
        """
        CLAUDE.md Rule: TODO/FIXME comments indicate incomplete work
        Found: Scanning for TODO, FIXME, HACK, XXX markers
        
        VIOLATION: Any TODO must have associated implementation or be removed
        """
        # Search for TODO/FIXME patterns in Python comments only (not markdown)
        # Split by code cell markers
        todo_pattern = re.compile(
            r'#\s*(TODO|FIXME|HACK|XXX|PLACEHOLDER)[\s:]',
            re.IGNORECASE
        )
        
        matches = list(todo_pattern.finditer(notebook_source))
        
        # Filter out HTML comments (from index.html artifacts)
        violations = []
        lines = notebook_source.split('\n')
        for match in matches:
            line_num = notebook_source[:match.start()].count('\n') + 1
            line_content = lines[line_num - 1].strip()
            
            # Skip HTML comments
            if not line_content.startswith('<!--'):
                violations.append(f"Line {line_num}: {line_content}")
        
        assert len(violations) == 0, (
            f"DECEPTION: Found {len(violations)} TODO/FIXME markers indicating incomplete work:\n" +
            "\n".join(violations)
        )

    def test_roadmap_has_verification_checkboxes(self, notebook_source):
        """
        CLAUDE.md Rule: Task lists must be verifiable with tests
        Found: Lines 832-851 - Implementation Roadmap with checkboxes
        
        VIOLATION: Each checkbox task must have corresponding verification test
        """
        # Extract roadmap section
        roadmap_pattern = re.compile(
            r'## \d+\. Implementation Roadmap(.*?)(?=##|\Z)',
            re.DOTALL
        )
        roadmap_match = roadmap_pattern.search(notebook_source)
        
        if roadmap_match:
            roadmap_section = roadmap_match.group(1)
            
            # Find all checkbox items
            checkbox_pattern = re.compile(r'- \[ \] (.+)')
            tasks = checkbox_pattern.findall(roadmap_section)
            
            # Each task should have testable acceptance criteria
            for task in tasks:
                # Tasks with vague language are suspicious
                vague_words = ['implement', 'add', 'create', 'develop']
                has_vague_only = any(word in task.lower() for word in vague_words)
                has_verification = any(word in task.lower() for word in ['verify', 'validate', 'test', 'benchmark'])
                
                # Not all tasks need verification words, but they should be specific
                if has_vague_only and not has_verification:
                    # Check if task is specific enough
                    is_specific = any(word in task.lower() for word in [
                        'pytorch', 'oscgrid', 'fault', 'downsample', 'multi-phase', 
                        'photonics', 'testbed', 'noise'
                    ])
                    
                    if not is_specific:
                        pytest.fail(
                            f"DECEPTION: Roadmap task is too vague without verification: '{task}'. "
                            f"Add specific acceptance criteria or test requirements."
                        )

    def test_no_empty_try_catch_blocks(self, notebook_source):
        """
        CLAUDE.md Rule: No empty try-catch blocks hiding errors
        Found: Scanning for exception handling
        
        VIOLATION: All exception handlers must log or handle errors properly
        """
        try:
            tree = ast.parse(notebook_source)
            
            for node in ast.walk(tree):
                if isinstance(node, ast.Try):
                    # Check if exception handlers are empty
                    for handler in node.handlers:
                        is_empty = len(handler.body) == 1 and isinstance(handler.body[0], ast.Pass)
                        only_pass = all(isinstance(stmt, ast.Pass) for stmt in handler.body)
                        
                        assert not (is_empty or only_pass), (
                            f"DECEPTION: Empty exception handler found. "
                            f"Must log error or provide meaningful handling."
                        )
        except SyntaxError:
            # Marimo notebooks may have mixed content, skip AST parsing
            pass

    def test_performance_metrics_verified_against_source(self, notebook_source):
        """
        CLAUDE.md Rule: Claims without verifiable implementations
        Found: Lines 746-756 - performance_metrics dictionary
        
        VIOLATION: Performance claims must be traceable to source data or papers
        """
        # Check for performance claims in markdown
        performance_claims = re.findall(
            r'(\d+\.?\d*)\s*(?:×|x|times|percent|%|mrad|pW|Hz)',
            notebook_source,
            re.IGNORECASE
        )
        
        # Should have references to papers or data sources
        has_arxiv_refs = 'arxiv' in notebook_source.lower()
        has_figshare_refs = 'figshare' in notebook_source.lower()
        has_data_refs = 'oscgrid' in notebook_source.lower()
        
        assert has_arxiv_refs or has_figshare_refs or has_data_refs, (
            "DECEPTION: Performance metrics claimed without references to source papers or data"
        )
        
        # Specifically check for MiRP paper reference
        assert '2504.16119' in notebook_source or 'arXiv:2504.16119' in notebook_source, (
            "DECEPTION: MiRP claims made without citing source paper arXiv:2504.16119"
        )

    def test_formulas_match_narrative_claims(self, notebook_source):
        """
        CLAUDE.md Rule: Mathematical operations must be shown, not just described
        Found: Multiple LaTeX formulas throughout
        
        VIOLATION: Every performance claim must have corresponding mathematical formula
        """
        # Find claims about accuracy, speedup, or performance
        claim_patterns = [
            r'(\d+)%\s+accuracy',
            r'(\d+)×\s+(?:speedup|improvement|faster)',
            r'(\d+)\s+pW',
            r'(\d+\.?\d*)\s+mrad'
        ]
        
        claims = []
        for pattern in claim_patterns:
            matches = re.findall(pattern, notebook_source, re.IGNORECASE)
            claims.extend(matches)
        
        # Count LaTeX formulas
        latex_formulas = re.findall(r'\$\$[^$]+\$\$', notebook_source)
        
        # Ratio check: should have at least one formula per 3 numerical claims
        formula_to_claim_ratio = len(latex_formulas) / max(len(claims), 1)
        
        assert formula_to_claim_ratio >= 0.3, (
            f"DECEPTION: {len(claims)} numerical claims but only {len(latex_formulas)} formulas. "
            f"Ratio {formula_to_claim_ratio:.2f} < 0.3. Must show mathematical derivations."
        )

    def test_integration_with_actual_data_files(self):
        """
        CLAUDE.md Rule: Test with real inputs, not examples
        Found: References to OscGrid CSV data
        
        VIOLATION: Must verify actual data files exist and are accessible
        """
        # Check for OscGrid data file references
        data_file_path = Path(__file__).parent.parent.parent
        
        # Look for CSV files in current directory or subdirectories
        csv_files = list(data_file_path.glob('**/*.csv'))
        
        # Should have actual data files, not just references in markdown
        # This is a soft warning - data may be external
        if len(csv_files) == 0:
            pytest.skip(
                "WARNING: No OscGrid CSV data files found. "
                "Analysis claims data integration but no local data verified. "
                "This may be acceptable if data is pulled from external source."
            )

    def test_no_commented_out_real_code(self, notebook_source):
        """
        CLAUDE.md Rule: No commented out "real" code with fake replacements
        Found: Scanning for suspicious comment patterns
        
        VIOLATION: Commented code suggests hidden real implementation
        """
        suspicious_comment_patterns = [
            r'#.*real implementation',
            r'#.*actual code',
            r'#.*production version',
            r'#.*uncomment for'
        ]
        
        violations = []
        for pattern in suspicious_comment_patterns:
            matches = re.finditer(pattern, notebook_source, re.IGNORECASE)
            for match in matches:
                line_num = notebook_source[:match.start()].count('\n') + 1
                violations.append(f"Line {line_num}: {match.group()}")
        
        assert len(violations) == 0, (
            f"DECEPTION: Found {len(violations)} suspicious comments suggesting hidden real code:\n" +
            "\n".join(violations)
        )


class TestFunctionalVerification:
    """Integration tests that actually execute code to verify it works"""

    def test_gaussian_filter_parameters_compute_correctly(self):
        """
        CLAUDE.md Rule: Edge case testing with real inputs
        Found: Lines 473-479 - Gaussian filter parameter calculation
        
        VERIFICATION: Execute calculation and verify dimensional correctness
        """
        f0 = 60  # Hz
        FWHM_cycles = 5
        FWHM_time = FWHM_cycles / f0
        sigma_time = FWHM_time / (2 * np.sqrt(2 * np.log(2)))
        T_eff = np.sqrt(2 * np.pi) * sigma_time
        
        # Verify dimensional correctness
        assert FWHM_time > 0 and FWHM_time < 1, (
            f"DECEPTION: FWHM_time = {FWHM_time} s is unrealistic for 60 Hz signal"
        )
        
        assert sigma_time > 0 and sigma_time < FWHM_time, (
            f"DECEPTION: sigma_time = {sigma_time} must be less than FWHM_time"
        )
        
        assert T_eff > 0 and T_eff < 1, (
            f"DECEPTION: T_eff = {T_eff} s is unrealistic for grid monitoring"
        )
        
        # Verify numerical values match expected range
        expected_T_eff_ms = 88.71  # from documentation
        assert abs(T_eff * 1000 - expected_T_eff_ms) < 0.1, (
            f"DECEPTION: T_eff = {T_eff*1000:.2f} ms doesn't match claimed {expected_T_eff_ms} ms"
        )

    def test_photon_counting_physics_is_valid(self):
        """
        CLAUDE.md Rule: Verify actual vs claimed functionality
        Found: Lines 500-504 - Photon counting calculation
        
        VERIFICATION: Execute and validate against quantum optics principles
        """
        h = 6.626e-34  # J·s
        c = 3e8  # m/s
        lambda_opt = 1550e-9  # m
        E_photon = h * c / lambda_opt
        
        P_opt = 1e-12  # 1 pW
        photon_rate = P_opt / E_photon
        T_eff = 88.71e-3  # seconds
        N_photons = photon_rate * T_eff
        delta_phi_SQL = 1 / np.sqrt(N_photons)
        
        # Verify photon energy is in correct range for telecom wavelength
        expected_E_photon = 1.28e-19  # J (approximate for 1550 nm)
        assert abs(E_photon - expected_E_photon) / expected_E_photon < 0.01, (
            f"DECEPTION: Photon energy {E_photon:.2e} J is incorrect for λ=1550nm"
        )
        
        # Verify photon rate is physically reasonable
        assert 1e6 < photon_rate < 1e8, (
            f"DECEPTION: Photon rate {photon_rate:.2e} photons/s is unrealistic for 1 pW"
        )
        
        # Verify SQL scaling
        assert N_photons > 0, "DECEPTION: Zero photons detected"
        assert delta_phi_SQL == pytest.approx(1/np.sqrt(N_photons), rel=1e-10), (
            "DECEPTION: SQL formula implementation is incorrect"
        )

    def test_improvement_factor_calculation_matches_claim(self):
        """
        CLAUDE.md Rule: Test edge cases and error conditions
        Found: Lines 724-730 - Improvement factor calculation
        
        VERIFICATION: Verify 26× improvement claim is correctly computed
        """
        measured_phase_noise = 26e-3  # rad
        target_phase = 1e-3  # rad
        improvement_factor = measured_phase_noise / target_phase
        
        assert improvement_factor == 26.0, (
            f"DECEPTION: Improvement factor {improvement_factor} doesn't match claimed 26×"
        )
        
        N_required_averaging = int(np.ceil(improvement_factor**2))
        
        # Verify averaging requirement
        expected_N = 676  # 26^2
        assert N_required_averaging == expected_N, (
            f"DECEPTION: Averaging samples {N_required_averaging} != expected {expected_N}"
        )


# Execution verification
if __name__ == "__main__":
    pytest.main([__file__, "-v", "--tb=short"])
