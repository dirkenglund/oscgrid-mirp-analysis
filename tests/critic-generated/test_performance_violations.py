"""
Performance Critic Tests for 20260204_OscGrid_Photonic_Quantum_Analysis.py

Generated by PERFORMANCE CRITIC agent
Date: 2026-02-05

Tests detect performance violations in the Marimo notebook implementation.
These tests should FAIL initially if performance issues exist.
"""

import pytest
import time
import numpy as np
from pathlib import Path
import sys
import ast
import re

# Path to the notebook file
NOTEBOOK_PATH = Path(__file__).parent.parent.parent / "20260204_OscGrid_Photonic_Quantum_Analysis.py"


def test_performance_violation_redundant_imports():
    """
    CLAUDE.md Rule: Avoid repeated imports - causes unnecessary module reloading
    Found: Lines 471, 656, 721 - numpy imported 3 times in different cells

    Violation: Same import statement repeated across multiple cells wastes time
    and memory. Imports should be consolidated in a single cell at the top.
    """
    with open(NOTEBOOK_PATH, 'r') as f:
        content = f.read()

    # Count numpy imports
    numpy_imports = re.findall(r'^\s*import numpy as np', content, re.MULTILINE)

    # Should have at most 1 numpy import (ideally in a shared imports cell)
    assert len(numpy_imports) <= 1, (
        f"Found {len(numpy_imports)} separate numpy imports. "
        f"Consolidate to a single import cell to avoid redundant module loading."
    )


def test_performance_violation_missing_caching_for_constants():
    """
    CLAUDE.md Rule: Cache repeated calculations, especially constants
    Found: Lines 481-508 - Physical constants recalculated in multiple cells

    Violation: Constants (h, c, lambda_opt, E_photon) are computed multiple times
    across cells. These should be computed once and cached.
    """
    with open(NOTEBOOK_PATH, 'r') as f:
        content = f.read()

    # Count how many times h = 6.626e-34 appears
    planck_constant_defs = re.findall(r'h\s*=\s*6\.626e-34', content)
    speed_of_light_defs = re.findall(r'c\s*=\s*3e8', content)

    # These constants should be defined once, not multiple times
    assert len(planck_constant_defs) <= 1, (
        f"Planck constant defined {len(planck_constant_defs)} times. "
        f"Define once in a constants cell and reuse."
    )
    assert len(speed_of_light_defs) <= 1, (
        f"Speed of light defined {len(speed_of_light_defs)} times. "
        f"Define once in a constants cell and reuse."
    )


def test_performance_violation_inefficient_loop_in_compute_output():
    """
    CLAUDE.md Rule: Avoid inefficient algorithms - vectorize when possible
    Found: Lines 301-314 - compute_output() uses sequential loop with odeint

    Violation: The loop iterates over W_weights and calls odeint for each weight
    separately. This should be vectorized or at least parallelize the loop.
    """
    # Parse the notebook to extract the compute_output function
    with open(NOTEBOOK_PATH, 'r') as f:
        content = f.read()

    # Check if compute_output exists and contains a loop over weights
    assert 'def compute_output' in content, "compute_output function not found"

    # Extract the function
    func_match = re.search(
        r'def compute_output\((.*?)\):\s*\n(.*?)(?=\n    def |\n@|\nif __name__|$)',
        content,
        re.DOTALL
    )

    if func_match:
        func_body = func_match.group(2)

        # Check for sequential loop pattern
        has_loop = 'for W in W_weights:' in func_body
        uses_odeint = 'odeint(' in func_body

        if has_loop and uses_odeint:
            # This is inefficient - should vectorize or parallelize
            pytest.fail(
                "compute_output() uses sequential loop with odeint. "
                "Vectorize using numpy broadcasting or use joblib.Parallel for speedup."
            )


def test_performance_violation_no_memoization():
    """
    CLAUDE.md Rule: Cache expensive computations that don't change
    Found: Entire notebook - no @functools.lru_cache or marimo caching

    Violation: Expensive numerical calculations (Gaussian filters, phase analysis)
    are recomputed every time cells execute. Should use memoization.
    """
    with open(NOTEBOOK_PATH, 'r') as f:
        content = f.read()

    # Check for any caching decorators
    has_lru_cache = '@lru_cache' in content or '@functools.lru_cache' in content
    has_marimo_cache = '@mo.cache' in content

    # For a notebook with expensive computations, should have at least some caching
    assert has_lru_cache or has_marimo_cache, (
        "No caching decorators found. Add @functools.lru_cache or @mo.cache "
        "to expensive pure functions to avoid redundant computation."
    )


def test_performance_violation_large_dict_creation_per_cell():
    """
    CLAUDE.md Rule: Avoid creating large objects in loops or repeatedly
    Found: Lines 510-523, 678-685, 746-756 - Large dicts created in cells

    Violation: Each cell creates dictionaries with computed values. If cells
    re-execute (common in notebooks), these are recreated wastefully.
    """
    with open(NOTEBOOK_PATH, 'r') as f:
        content = f.read()

    # Count dictionary definitions with many keys (>5 indicates "large")
    dict_patterns = re.findall(r'(\w+)\s*=\s*\{[^}]{100,}\}', content)

    # Should consolidate into a cached computation or lazy evaluation
    if len(dict_patterns) > 3:
        pytest.fail(
            f"Found {len(dict_patterns)} large dictionary creations. "
            f"Consider using @mo.cache or lazy evaluation to avoid recreating "
            f"these on every cell execution."
        )


def test_performance_violation_missing_lazy_data_loading():
    """
    CLAUDE.md Rule: Use lazy loading for large datasets
    Found: Documentation references 384 MB labeled.csv but no lazy loading pattern

    Violation: Markdown mentions large dataset but no code shows chunked reading
    or lazy iteration. Loading 384 MB at once is inefficient.
    """
    with open(NOTEBOOK_PATH, 'r') as f:
        content = f.read()

    # Check if pandas read_csv with chunking is used
    has_chunking = 'chunksize=' in content
    has_iterator = 'iterator=' in content
    has_lazy_loading = has_chunking or has_iterator

    # Check if the dataset is mentioned
    mentions_oscgrid = 'labeled.csv' in content or 'OscGrid' in content

    if mentions_oscgrid and not has_lazy_loading:
        pytest.fail(
            "Mentions OscGrid dataset (384 MB) but no chunked/lazy loading detected. "
            "Use pd.read_csv(..., chunksize=10000) or iterator=True for large files."
        )


def test_performance_violation_repeated_sqrt_calls():
    """
    CLAUDE.md Rule: Precompute expensive operations when used multiple times
    Found: Multiple calls to np.sqrt() with same arguments

    Violation: np.sqrt(2 * np.log(2)) appears multiple times in calculations.
    Should precompute sqrt_2ln2 = np.sqrt(2 * np.log(2)) once.
    """
    with open(NOTEBOOK_PATH, 'r') as f:
        content = f.read()

    # Pattern for sqrt(2 * log(2)) or sqrt(2*log(2))
    sqrt_pattern = r'np\.sqrt\(2\s*\*\s*np\.log\(2\)\)'
    matches = re.findall(sqrt_pattern, content)

    if len(matches) > 1:
        pytest.fail(
            f"Found {len(matches)} calls to np.sqrt(2 * np.log(2)). "
            f"Precompute this constant once as SQRT_2LN2 to avoid repeated calculation."
        )


def test_performance_violation_inefficient_string_formatting():
    """
    CLAUDE.md Rule: Use efficient string formatting for dynamic content
    Found: Lines 529-541 - f-string in mo.md() regenerates on every cell run

    Violation: Formatted markdown strings regenerate even when data unchanged.
    Should cache rendered markdown or use reactive dependencies properly.
    """
    with open(NOTEBOOK_PATH, 'r') as f:
        content = f.read()

    # Count mo.md(f""" patterns
    fstring_markdown = re.findall(r'mo\.md\(f"""', content)

    # Too many dynamic markdown cells can cause performance issues
    if len(fstring_markdown) > 5:
        pytest.warn(
            pytest.PytestWarning(
                f"Found {len(fstring_markdown)} dynamic markdown cells with f-strings. "
                f"Consider making markdown static where data doesn't change frequently."
            )
        )


def test_performance_violation_missing_vectorization():
    """
    CLAUDE.md Rule: Use numpy vectorization instead of Python loops
    Found: Potential loop-based calculations instead of array operations

    Violation: Any Python for-loop over numerical data should use numpy operations
    """
    with open(NOTEBOOK_PATH, 'r') as f:
        content = f.read()

    # Look for patterns like: for i in range(...): array[i] = ...
    loop_assignment_pattern = r'for \w+ in range\([^)]+\):\s*\n\s+\w+\[\w+\]\s*='
    matches = re.findall(loop_assignment_pattern, content)

    if matches:
        pytest.fail(
            f"Found {len(matches)} potential loop-based array assignments. "
            f"Use numpy vectorized operations for better performance."
        )


def test_performance_actual_runtime_benchmark():
    """
    CLAUDE.md Rule: Performance tests should verify actual execution time

    Simulates loading and executing key cells to measure actual performance.
    This test measures if expensive computations exceed acceptable thresholds.
    """
    # Test that phase_analysis computation completes within 100ms
    start = time.time()

    # Replicate the expensive calculation from line 481-524
    f0 = 60
    FWHM_cycles = 5
    FWHM_time = FWHM_cycles / f0
    sigma_time = FWHM_time / (2 * np.sqrt(2 * np.log(2)))
    T_eff = np.sqrt(2 * np.pi) * sigma_time

    h = 6.626e-34
    c = 3e8
    lambda_opt = 1550e-9
    E_photon = h * c / lambda_opt

    B_pp = 1e-6
    B_0 = B_pp / 2
    omega = 2 * np.pi * f0

    N_turns = 1000
    A_coil = 0.01
    V_peak = N_turns * A_coil * omega * B_0

    V_pi = 3.0
    delta_phi_mod = np.pi * V_peak / V_pi

    P_opt = 1e-12
    photon_rate = P_opt / E_photon
    N_photons = photon_rate * T_eff
    delta_phi_SQL = 1 / np.sqrt(N_photons)

    elapsed = time.time() - start

    # Should complete in under 100ms (it's just arithmetic)
    assert elapsed < 0.1, (
        f"Phase analysis took {elapsed*1000:.1f}ms. "
        f"Should be < 100ms for basic numpy operations. Check for inefficiencies."
    )


def test_performance_memory_efficiency():
    """
    CLAUDE.md Rule: Avoid loading too much data into memory at once

    Tests that data structures don't consume excessive memory
    """
    # Simulate creating the large dictionaries from the notebook
    phase_analysis = {
        "f0_Hz": 60,
        "FWHM_ms": 83.33,
        "sigma_ms": 35.39,
        "T_eff_ms": 88.71,
        "B_pp_uT": 1.0,
        "V_peak_mV": 1.88,
        "delta_phi_mod_mrad": 1.97,
        "N_photons": 690000,
        "delta_phi_SQL_rad": 1.24e-3,
        "target_rad": 1e-3,
        "achievable": True
    }

    dqs_comparison = {
        "SQL_measurements": 1000000,
        "Heisenberg_sensors": 1000,
        "MiRP_power_pW": 1.5,
        "MiRP_photons": 1000000,
        "target_rad": 1e-3
    }

    performance_metrics = {
        "measured_noise_mrad": 26.0,
        "target_mrad": 1.0,
        "improvement_needed": 26.0,
        "averaging_samples_needed": 676,
        "averaging_time_ms": 420.0,
        "sensor_nodes_needed": 26,
        "raw_bandwidth_Hz": 801.0,
        "averaged_bandwidth_Hz": 2.38
    }

    # These should be small dicts (< 1 KB each)
    import sys
    size_phase = sys.getsizeof(phase_analysis)
    size_dqs = sys.getsizeof(dqs_comparison)
    size_perf = sys.getsizeof(performance_metrics)

    total_size = size_phase + size_dqs + size_perf

    # Should be well under 10 KB for these simple dicts
    assert total_size < 10000, (
        f"Total memory for result dicts: {total_size} bytes. "
        f"Consider using numpy arrays or dataclasses for large structures."
    )


if __name__ == "__main__":
    pytest.main([__file__, "-v", "--tb=short"])
